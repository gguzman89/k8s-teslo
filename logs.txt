
==> Audit <==
|---------|--------------------------------|----------|------|---------|---------------------|---------------------|
| Command |              Args              | Profile  | User | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|------|---------|---------------------|---------------------|
| start   |                                | minikube | root | v1.33.1 | 08 Aug 24 12:15 -03 |                     |
| start   | --vm-driver=none               | minikube | root | v1.33.1 | 08 Aug 24 12:18 -03 |                     |
|         | --driver=docker                |          |      |         |                     |                     |
| start   | --vm-driver=none               | minikube | root | v1.33.1 | 08 Aug 24 12:18 -03 | 08 Aug 24 12:23 -03 |
|         | --driver=docker --force        |          |      |         |                     |                     |
| service | pg-admin-service               | minikube | root | v1.33.1 | 09 Aug 24 09:08 -03 |                     |
|---------|--------------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/08/08 12:18:30
Running on machine: DESKTOP-TP554KH
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0808 12:18:30.799695   31277 out.go:291] Setting OutFile to fd 1 ...
I0808 12:18:30.799923   31277 out.go:343] isatty.IsTerminal(1) = true
I0808 12:18:30.799928   31277 out.go:304] Setting ErrFile to fd 2...
I0808 12:18:30.799933   31277 out.go:343] isatty.IsTerminal(2) = true
I0808 12:18:30.800196   31277 root.go:338] Updating PATH: /root/.minikube/bin
W0808 12:18:30.800337   31277 root.go:314] Error reading config file at /root/.minikube/config/config.json: open /root/.minikube/config/config.json: no such file or directory
I0808 12:18:30.800534   31277 out.go:298] Setting JSON to false
I0808 12:18:30.801458   31277 start.go:129] hostinfo: {"hostname":"DESKTOP-TP554KH","uptime":5816,"bootTime":1723124495,"procs":73,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.153.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"df87d02c-17b5-46ea-a186-09b0191e52e5"}
I0808 12:18:30.801528   31277 start.go:139] virtualization:  guest
I0808 12:18:30.804789   31277 out.go:177] üòÑ  minikube v1.33.1 on Ubuntu 22.04 (amd64)
W0808 12:18:30.811587   31277 out.go:239] ‚ùó  minikube skips various validations when --force is supplied; this may lead to unexpected behavior
W0808 12:18:30.811660   31277 preload.go:294] Failed to list preload files: open /root/.minikube/cache/preloaded-tarball: no such file or directory
I0808 12:18:30.811696   31277 notify.go:220] Checking for updates...
I0808 12:18:30.811920   31277 driver.go:392] Setting default libvirt URI to qemu:///system
W0808 12:18:30.812007   31277 out.go:239] ‚ùó  Both driver=docker and vm-driver=none have been set.

    Since vm-driver is deprecated, minikube will default to driver=docker.

    If vm-driver is set in the global config, please run "minikube config unset vm-driver" to resolve this warning.
			
I0808 12:18:30.898880   31277 docker.go:122] docker version: linux-26.1.1:Docker Desktop
I0808 12:18:30.898956   31277 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0808 12:18:31.064548   31277 info.go:266] docker info: {ID:fed4441a-dbb9-4e58-acd6-66230f0760b0 Containers:41 ContainersRunning:1 ContainersPaused:0 ContainersStopped:40 Images:44 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:78 SystemTime:2024-08-08 15:18:31.044847129 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8267890688 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-scan: no such file or directory Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0808 12:18:31.064665   31277 docker.go:295] overlay module found
I0808 12:18:31.066873   31277 out.go:177] ‚ú®  Using the docker driver based on user configuration
I0808 12:18:31.069125   31277 start.go:297] selected driver: docker
I0808 12:18:31.069135   31277 start.go:901] validating driver "docker" against <nil>
I0808 12:18:31.069167   31277 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
W0808 12:18:31.069311   31277 out.go:239] üõë  The "docker" driver should not be used with root privileges. If you wish to continue as root, use --force.
W0808 12:18:31.069377   31277 out.go:239] üí°  If you are running minikube within a VM, consider using --driver=none:
W0808 12:18:31.069403   31277 out.go:239] üìò    https://minikube.sigs.k8s.io/docs/reference/drivers/none/
I0808 12:18:31.069576   31277 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0808 12:18:31.264423   31277 info.go:266] docker info: {ID:fed4441a-dbb9-4e58-acd6-66230f0760b0 Containers:41 ContainersRunning:1 ContainersPaused:0 ContainersStopped:40 Images:44 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:78 SystemTime:2024-08-08 15:18:31.223314823 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8267890688 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-scan: no such file or directory Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0808 12:18:31.264664   31277 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0808 12:18:31.265269   31277 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=7884MB, container=7884MB
I0808 12:18:31.265603   31277 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0808 12:18:31.268314   31277 out.go:177] üìå  Using Docker driver with root privileges
W0808 12:18:31.280587   31277 out.go:239] ‚ùó  For an improved experience it's recommended to use Docker Engine instead of Docker Desktop.
Docker Engine installation instructions: https://docs.docker.com/engine/install/#server
I0808 12:18:31.280686   31277 cni.go:84] Creating CNI manager for ""
I0808 12:18:31.280702   31277 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0808 12:18:31.280712   31277 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0808 12:18:31.280800   31277 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0808 12:18:31.283591   31277 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0808 12:18:31.291389   31277 cache.go:121] Beginning downloading kic base image for docker with docker
I0808 12:18:31.293516   31277 out.go:177] üöú  Pulling base image v0.0.44 ...
I0808 12:18:31.298237   31277 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0808 12:18:31.298257   31277 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0808 12:18:31.397200   31277 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e to local cache
I0808 12:18:31.398038   31277 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local cache directory
I0808 12:18:31.398756   31277 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e to local cache
I0808 12:18:31.489740   31277 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0808 12:18:31.489760   31277 cache.go:56] Caching tarball of preloaded images
I0808 12:18:31.489969   31277 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0808 12:18:31.520245   31277 out.go:177] üíæ  Downloading Kubernetes v1.30.0 preload ...
I0808 12:18:31.542449   31277 preload.go:237] getting checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0808 12:18:31.865916   31277 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4?checksum=md5:00b6acf85a82438f3897c0a6fafdcee7 -> /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0808 12:21:08.341905   31277 preload.go:248] saving checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0808 12:21:08.341991   31277 preload.go:255] verifying checksum of /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0808 12:21:09.798390   31277 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0808 12:21:09.798773   31277 profile.go:143] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0808 12:21:09.798797   31277 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/config.json: {Name:mk270d1b5db5965f2dc9e9e25770a63417031943 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:21:30.465343   31277 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e as a tarball
I0808 12:21:30.465351   31277 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from local cache
I0808 12:21:31.152420   31277 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from cached tarball
I0808 12:21:31.152463   31277 cache.go:194] Successfully downloaded all kic artifacts
I0808 12:21:31.153368   31277 start.go:360] acquireMachinesLock for minikube: {Name:mke11f63b5835bf422927bf558fccac7a21a838f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0808 12:21:31.153569   31277 start.go:364] duration metric: took 168.01¬µs to acquireMachinesLock for "minikube"
I0808 12:21:31.153598   31277 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0808 12:21:31.153677   31277 start.go:125] createHost starting for "" (driver="docker")
I0808 12:21:31.158126   31277 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0808 12:21:31.160573   31277 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0808 12:21:31.160698   31277 client.go:168] LocalClient.Create starting
I0808 12:21:31.163305   31277 main.go:141] libmachine: Creating CA: /root/.minikube/certs/ca.pem
I0808 12:21:31.784455   31277 main.go:141] libmachine: Creating client certificate: /root/.minikube/certs/cert.pem
I0808 12:21:32.089353   31277 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0808 12:21:32.327197   31277 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0808 12:21:32.327327   31277 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0808 12:21:32.327344   31277 cli_runner.go:164] Run: docker network inspect minikube
W0808 12:21:32.428196   31277 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0808 12:21:32.428227   31277 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0808 12:21:32.428238   31277 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0808 12:21:32.428804   31277 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0808 12:21:32.554068   31277 network.go:209] skipping subnet 192.168.49.0/24 that is reserved: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:<nil>}
I0808 12:21:32.554339   31277 network.go:206] using free private subnet 192.168.58.0/24: &{IP:192.168.58.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.58.0/24 Gateway:192.168.58.1 ClientMin:192.168.58.2 ClientMax:192.168.58.254 Broadcast:192.168.58.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0012eaea0}
I0808 12:21:32.554354   31277 network_create.go:124] attempt to create docker network minikube 192.168.58.0/24 with gateway 192.168.58.1 and MTU of 1500 ...
I0808 12:21:32.554411   31277 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.58.0/24 --gateway=192.168.58.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0808 12:21:32.753515   31277 network_create.go:108] docker network minikube 192.168.58.0/24 created
I0808 12:21:32.753551   31277 kic.go:121] calculated static IP "192.168.58.2" for the "minikube" container
I0808 12:21:32.753646   31277 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0808 12:21:32.865537   31277 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0808 12:21:33.103748   31277 oci.go:103] Successfully created a docker volume minikube
I0808 12:21:33.103819   31277 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib
I0808 12:21:36.811955   31277 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib: (3.708099588s)
I0808 12:21:36.811975   31277 oci.go:107] Successfully prepared a docker volume minikube
I0808 12:21:36.812013   31277 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0808 12:21:36.812049   31277 kic.go:194] Starting extracting preloaded images to volume ...
I0808 12:21:36.812115   31277 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir
I0808 12:22:39.665591   31277 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /root/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir: (1m2.855805238s)
I0808 12:22:39.665613   31277 kic.go:203] duration metric: took 1m2.855955047s to extract preloaded images to volume ...
W0808 12:22:39.666246   31277 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I0808 12:22:39.666399   31277 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0808 12:22:40.880799   31277 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.214375543s)
I0808 12:22:40.880938   31277 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.58.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e
I0808 12:22:42.932787   31277 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.58.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e: (2.051794655s)
I0808 12:22:42.932860   31277 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0808 12:22:43.282155   31277 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0808 12:22:43.722279   31277 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0808 12:22:44.784286   31277 cli_runner.go:217] Completed: docker exec minikube stat /var/lib/dpkg/alternatives/iptables: (1.07111971s)
I0808 12:22:44.784306   31277 oci.go:144] the created container "minikube" has a running status.
I0808 12:22:44.784319   31277 kic.go:225] Creating ssh key for kic: /root/.minikube/machines/minikube/id_rsa...
I0808 12:22:45.477550   31277 kic_runner.go:191] docker (temp): /root/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0808 12:22:45.797330   31277 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0808 12:22:46.250388   31277 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0808 12:22:46.250399   31277 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0808 12:22:46.632535   31277 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0808 12:22:47.061893   31277 machine.go:94] provisionDockerMachine start ...
I0808 12:22:47.062091   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:47.182629   31277 main.go:141] libmachine: Using SSH client type: native
I0808 12:22:47.183080   31277 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 60569 <nil> <nil>}
I0808 12:22:47.183088   31277 main.go:141] libmachine: About to run SSH command:
hostname
I0808 12:22:47.497755   31277 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0808 12:22:47.500044   31277 ubuntu.go:169] provisioning hostname "minikube"
I0808 12:22:47.500149   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:47.662211   31277 main.go:141] libmachine: Using SSH client type: native
I0808 12:22:47.662442   31277 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 60569 <nil> <nil>}
I0808 12:22:47.662451   31277 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0808 12:22:48.251884   31277 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0808 12:22:48.251957   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:48.518459   31277 main.go:141] libmachine: Using SSH client type: native
I0808 12:22:48.518700   31277 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 60569 <nil> <nil>}
I0808 12:22:48.518714   31277 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0808 12:22:48.822566   31277 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0808 12:22:48.822584   31277 ubuntu.go:175] set auth options {CertDir:/root/.minikube CaCertPath:/root/.minikube/certs/ca.pem CaPrivateKeyPath:/root/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/root/.minikube/machines/server.pem ServerKeyPath:/root/.minikube/machines/server-key.pem ClientKeyPath:/root/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/root/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/root/.minikube}
I0808 12:22:48.822597   31277 ubuntu.go:177] setting up certificates
I0808 12:22:48.822609   31277 provision.go:84] configureAuth start
I0808 12:22:48.822667   31277 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0808 12:22:49.281416   31277 provision.go:143] copyHostCerts
I0808 12:22:49.281481   31277 exec_runner.go:151] cp: /root/.minikube/certs/key.pem --> /root/.minikube/key.pem (1679 bytes)
I0808 12:22:49.281636   31277 exec_runner.go:151] cp: /root/.minikube/certs/ca.pem --> /root/.minikube/ca.pem (1070 bytes)
I0808 12:22:49.281716   31277 exec_runner.go:151] cp: /root/.minikube/certs/cert.pem --> /root/.minikube/cert.pem (1115 bytes)
I0808 12:22:49.281782   31277 provision.go:117] generating server cert: /root/.minikube/machines/server.pem ca-key=/root/.minikube/certs/ca.pem private-key=/root/.minikube/certs/ca-key.pem org=root.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0808 12:22:49.471455   31277 provision.go:177] copyRemoteCerts
I0808 12:22:49.471603   31277 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0808 12:22:49.471662   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:49.700718   31277 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60569 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0808 12:22:49.831074   31277 ssh_runner.go:362] scp /root/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0808 12:22:49.880732   31277 ssh_runner.go:362] scp /root/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0808 12:22:49.927137   31277 ssh_runner.go:362] scp /root/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0808 12:22:49.977784   31277 provision.go:87] duration metric: took 1.155163939s to configureAuth
I0808 12:22:49.977802   31277 ubuntu.go:193] setting minikube options for container-runtime
I0808 12:22:49.977998   31277 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0808 12:22:49.978062   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:50.091794   31277 main.go:141] libmachine: Using SSH client type: native
I0808 12:22:50.091987   31277 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 60569 <nil> <nil>}
I0808 12:22:50.091995   31277 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0808 12:22:50.332247   31277 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0808 12:22:50.332260   31277 ubuntu.go:71] root file system type: overlay
I0808 12:22:50.332390   31277 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0808 12:22:50.332464   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:50.554829   31277 main.go:141] libmachine: Using SSH client type: native
I0808 12:22:50.555051   31277 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 60569 <nil> <nil>}
I0808 12:22:50.555115   31277 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0808 12:22:50.788834   31277 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0808 12:22:50.788999   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:50.936290   31277 main.go:141] libmachine: Using SSH client type: native
I0808 12:22:50.936599   31277 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 60569 <nil> <nil>}
I0808 12:22:50.936621   31277 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0808 12:22:53.389660   31277 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-30 11:46:26.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-08-08 15:22:50.779570738 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0808 12:22:53.389690   31277 machine.go:97] duration metric: took 6.115514454s to provisionDockerMachine
I0808 12:22:53.389698   31277 client.go:171] duration metric: took 1m22.028263815s to LocalClient.Create
I0808 12:22:53.389755   31277 start.go:167] duration metric: took 1m22.028455826s to libmachine.API.Create "minikube"
I0808 12:22:53.389762   31277 start.go:293] postStartSetup for "minikube" (driver="docker")
I0808 12:22:53.389772   31277 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0808 12:22:53.391457   31277 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0808 12:22:53.391510   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:53.502654   31277 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60569 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0808 12:22:53.648878   31277 ssh_runner.go:195] Run: cat /etc/os-release
I0808 12:22:53.656139   31277 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0808 12:22:53.656161   31277 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0808 12:22:53.656169   31277 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0808 12:22:53.656175   31277 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0808 12:22:53.656192   31277 filesync.go:126] Scanning /root/.minikube/addons for local assets ...
I0808 12:22:53.656260   31277 filesync.go:126] Scanning /root/.minikube/files for local assets ...
I0808 12:22:53.656289   31277 start.go:296] duration metric: took 266.521528ms for postStartSetup
I0808 12:22:53.656646   31277 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0808 12:22:53.821940   31277 profile.go:143] Saving config to /root/.minikube/profiles/minikube/config.json ...
I0808 12:22:53.822250   31277 start.go:128] duration metric: took 1m22.4678247s to createHost
I0808 12:22:53.822260   31277 start.go:83] releasing machines lock for "minikube", held for 1m22.467950408s
I0808 12:22:53.824134   31277 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0808 12:22:54.028573   31277 ssh_runner.go:195] Run: cat /version.json
I0808 12:22:54.028646   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:54.029668   31277 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0808 12:22:54.029754   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:22:54.233331   31277 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60569 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0808 12:22:54.281322   31277 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60569 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0808 12:22:54.837380   31277 ssh_runner.go:195] Run: systemctl --version
I0808 12:22:54.847853   31277 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0808 12:22:54.862651   31277 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0808 12:22:54.920226   31277 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0808 12:22:54.920299   31277 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0808 12:22:55.147577   31277 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0808 12:22:55.147593   31277 start.go:494] detecting cgroup driver to use...
I0808 12:22:55.147632   31277 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0808 12:22:55.161018   31277 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0808 12:22:55.230094   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0808 12:22:55.255615   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0808 12:22:55.302215   31277 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0808 12:22:55.302326   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0808 12:22:55.334883   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0808 12:22:55.375860   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0808 12:22:55.398284   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0808 12:22:55.429954   31277 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0808 12:22:55.451325   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0808 12:22:55.479300   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0808 12:22:55.504256   31277 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0808 12:22:55.520930   31277 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0808 12:22:55.550824   31277 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0808 12:22:55.565870   31277 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0808 12:22:55.766414   31277 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0808 12:22:56.060024   31277 start.go:494] detecting cgroup driver to use...
I0808 12:22:56.060116   31277 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0808 12:22:56.060233   31277 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0808 12:22:56.118943   31277 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0808 12:22:56.118999   31277 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0808 12:22:56.190252   31277 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0808 12:22:56.252229   31277 ssh_runner.go:195] Run: which cri-dockerd
I0808 12:22:56.260364   31277 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0808 12:22:56.284825   31277 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0808 12:22:56.332732   31277 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0808 12:22:56.543935   31277 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0808 12:22:56.737771   31277 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0808 12:22:56.737891   31277 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0808 12:22:56.768459   31277 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0808 12:22:56.938701   31277 ssh_runner.go:195] Run: sudo systemctl restart docker
I0808 12:22:59.063823   31277 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.125100191s)
I0808 12:22:59.063896   31277 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0808 12:22:59.120136   31277 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0808 12:22:59.211670   31277 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0808 12:22:59.534536   31277 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0808 12:22:59.799282   31277 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0808 12:23:00.000628   31277 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0808 12:23:00.035514   31277 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0808 12:23:00.085745   31277 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0808 12:23:00.294881   31277 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0808 12:23:00.600402   31277 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0808 12:23:00.600525   31277 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0808 12:23:00.616740   31277 start.go:562] Will wait 60s for crictl version
I0808 12:23:00.616813   31277 ssh_runner.go:195] Run: which crictl
I0808 12:23:00.661049   31277 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0808 12:23:00.937518   31277 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0808 12:23:00.937592   31277 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0808 12:23:01.017217   31277 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0808 12:23:01.137509   31277 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0808 12:23:01.137647   31277 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0808 12:23:01.288352   31277 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0808 12:23:01.293761   31277 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0808 12:23:01.323093   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0808 12:23:01.512076   31277 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0808 12:23:01.512217   31277 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0808 12:23:01.512319   31277 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0808 12:23:01.543766   31277 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0808 12:23:01.543778   31277 docker.go:615] Images already preloaded, skipping extraction
I0808 12:23:01.543841   31277 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0808 12:23:01.570502   31277 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0808 12:23:01.570518   31277 cache_images.go:84] Images are preloaded, skipping loading
I0808 12:23:01.570526   31277 kubeadm.go:928] updating node { 192.168.58.2 8443 v1.30.0 docker true true} ...
I0808 12:23:01.570612   31277 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0808 12:23:01.570696   31277 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0808 12:23:01.748852   31277 cni.go:84] Creating CNI manager for ""
I0808 12:23:01.748885   31277 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0808 12:23:01.748898   31277 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0808 12:23:01.748923   31277 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0808 12:23:01.756592   31277 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0808 12:23:01.756768   31277 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0808 12:23:01.788314   31277 binaries.go:44] Found k8s binaries, skipping transfer
I0808 12:23:01.788361   31277 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0808 12:23:01.867876   31277 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0808 12:23:01.971594   31277 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0808 12:23:02.057035   31277 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0808 12:23:02.105827   31277 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0808 12:23:02.115060   31277 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0808 12:23:02.165928   31277 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0808 12:23:02.316482   31277 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0808 12:23:02.353114   31277 certs.go:68] Setting up /root/.minikube/profiles/minikube for IP: 192.168.58.2
I0808 12:23:02.353126   31277 certs.go:194] generating shared ca certs ...
I0808 12:23:02.353144   31277 certs.go:226] acquiring lock for ca certs: {Name:mkb814c315fe9b7fabb439d6d58c5448fbb7853c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:02.353273   31277 certs.go:240] generating "minikubeCA" ca cert: /root/.minikube/ca.key
I0808 12:23:02.629049   31277 crypto.go:156] Writing cert to /root/.minikube/ca.crt ...
I0808 12:23:02.629069   31277 lock.go:35] WriteFile acquiring /root/.minikube/ca.crt: {Name:mk748923f05408fc6b894e88d2f62f90f80f521f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:02.629309   31277 crypto.go:164] Writing key to /root/.minikube/ca.key ...
I0808 12:23:02.629316   31277 lock.go:35] WriteFile acquiring /root/.minikube/ca.key: {Name:mkadab1dda45ad4e9b64e75e19812de132576577 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:02.629500   31277 certs.go:240] generating "proxyClientCA" ca cert: /root/.minikube/proxy-client-ca.key
I0808 12:23:03.247874   31277 crypto.go:156] Writing cert to /root/.minikube/proxy-client-ca.crt ...
I0808 12:23:03.247892   31277 lock.go:35] WriteFile acquiring /root/.minikube/proxy-client-ca.crt: {Name:mkff7217c4d17525a6f0f7c4ccc47f0953a17f51 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:03.248095   31277 crypto.go:164] Writing key to /root/.minikube/proxy-client-ca.key ...
I0808 12:23:03.248103   31277 lock.go:35] WriteFile acquiring /root/.minikube/proxy-client-ca.key: {Name:mkd65fc0723375be0692aee2901eb2e57ab2760f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:03.248195   31277 certs.go:256] generating profile certs ...
I0808 12:23:03.248258   31277 certs.go:363] generating signed profile cert for "minikube-user": /root/.minikube/profiles/minikube/client.key
I0808 12:23:03.249443   31277 crypto.go:68] Generating cert /root/.minikube/profiles/minikube/client.crt with IP's: []
I0808 12:23:03.498963   31277 crypto.go:156] Writing cert to /root/.minikube/profiles/minikube/client.crt ...
I0808 12:23:03.498981   31277 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/client.crt: {Name:mk09878e812b07af637940656ec44996daba95aa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:03.499150   31277 crypto.go:164] Writing key to /root/.minikube/profiles/minikube/client.key ...
I0808 12:23:03.499157   31277 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/client.key: {Name:mkf3b978f9858871583d8228f83a87a85b7d106f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:03.499305   31277 certs.go:363] generating signed profile cert for "minikube": /root/.minikube/profiles/minikube/apiserver.key.502bbb95
I0808 12:23:03.499321   31277 crypto.go:68] Generating cert /root/.minikube/profiles/minikube/apiserver.crt.502bbb95 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.58.2]
I0808 12:23:03.768372   31277 crypto.go:156] Writing cert to /root/.minikube/profiles/minikube/apiserver.crt.502bbb95 ...
I0808 12:23:03.768390   31277 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/apiserver.crt.502bbb95: {Name:mk44ae1c923a85ead009c8daf31afdbbbfa5c64e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:03.768577   31277 crypto.go:164] Writing key to /root/.minikube/profiles/minikube/apiserver.key.502bbb95 ...
I0808 12:23:03.768584   31277 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/apiserver.key.502bbb95: {Name:mk47f21c888a54239fce2effecebf9a5a08f06c6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:03.768673   31277 certs.go:381] copying /root/.minikube/profiles/minikube/apiserver.crt.502bbb95 -> /root/.minikube/profiles/minikube/apiserver.crt
I0808 12:23:03.779272   31277 certs.go:385] copying /root/.minikube/profiles/minikube/apiserver.key.502bbb95 -> /root/.minikube/profiles/minikube/apiserver.key
I0808 12:23:03.779462   31277 certs.go:363] generating signed profile cert for "aggregator": /root/.minikube/profiles/minikube/proxy-client.key
I0808 12:23:03.779490   31277 crypto.go:68] Generating cert /root/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0808 12:23:03.920696   31277 crypto.go:156] Writing cert to /root/.minikube/profiles/minikube/proxy-client.crt ...
I0808 12:23:03.920714   31277 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/proxy-client.crt: {Name:mkcab3ddb18cd096d978df14d87a44e804896057 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:03.920994   31277 crypto.go:164] Writing key to /root/.minikube/profiles/minikube/proxy-client.key ...
I0808 12:23:03.921001   31277 lock.go:35] WriteFile acquiring /root/.minikube/profiles/minikube/proxy-client.key: {Name:mkaff5bf6f623f02423597918f5f33c2a99a3db1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:03.921263   31277 certs.go:484] found cert: /root/.minikube/certs/ca-key.pem (1679 bytes)
I0808 12:23:03.921303   31277 certs.go:484] found cert: /root/.minikube/certs/ca.pem (1070 bytes)
I0808 12:23:03.921334   31277 certs.go:484] found cert: /root/.minikube/certs/cert.pem (1115 bytes)
I0808 12:23:03.921369   31277 certs.go:484] found cert: /root/.minikube/certs/key.pem (1679 bytes)
I0808 12:23:04.273682   31277 ssh_runner.go:362] scp /root/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0808 12:23:04.429282   31277 ssh_runner.go:362] scp /root/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0808 12:23:04.490024   31277 ssh_runner.go:362] scp /root/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0808 12:23:04.535600   31277 ssh_runner.go:362] scp /root/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0808 12:23:04.595916   31277 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0808 12:23:04.650799   31277 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0808 12:23:04.703198   31277 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0808 12:23:04.788571   31277 ssh_runner.go:362] scp /root/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0808 12:23:04.881960   31277 ssh_runner.go:362] scp /root/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0808 12:23:04.966764   31277 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0808 12:23:05.006592   31277 ssh_runner.go:195] Run: openssl version
I0808 12:23:05.022188   31277 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0808 12:23:05.049801   31277 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0808 12:23:05.056815   31277 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Aug  8 15:23 /usr/share/ca-certificates/minikubeCA.pem
I0808 12:23:05.056865   31277 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0808 12:23:05.076249   31277 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0808 12:23:05.101986   31277 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0808 12:23:05.117498   31277 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0808 12:23:05.117546   31277 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/root:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0808 12:23:05.118564   31277 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0808 12:23:05.156171   31277 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0808 12:23:05.176154   31277 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0808 12:23:05.232287   31277 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0808 12:23:05.245076   31277 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0808 12:23:05.287045   31277 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0808 12:23:05.301986   31277 kubeadm.go:156] found existing configuration files:

I0808 12:23:05.302044   31277 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0808 12:23:05.321169   31277 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0808 12:23:05.321232   31277 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0808 12:23:05.347811   31277 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0808 12:23:05.363268   31277 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0808 12:23:05.363315   31277 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0808 12:23:05.395604   31277 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0808 12:23:05.444547   31277 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0808 12:23:05.458970   31277 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0808 12:23:05.499568   31277 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0808 12:23:05.522755   31277 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0808 12:23:05.522834   31277 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0808 12:23:05.564178   31277 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0808 12:23:06.250701   31277 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0808 12:23:06.250763   31277 kubeadm.go:309] [preflight] Running pre-flight checks
I0808 12:23:06.779926   31277 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0808 12:23:06.780029   31277 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0808 12:23:06.780139   31277 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0808 12:23:07.465952   31277 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0808 12:23:07.477554   31277 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0808 12:23:07.485383   31277 kubeadm.go:309] [certs] Using existing ca certificate authority
I0808 12:23:07.485472   31277 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0808 12:23:07.867315   31277 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0808 12:23:08.354251   31277 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0808 12:23:08.968252   31277 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0808 12:23:09.869329   31277 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0808 12:23:10.124835   31277 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0808 12:23:10.125351   31277 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.58.2 127.0.0.1 ::1]
I0808 12:23:10.212661   31277 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0808 12:23:10.212797   31277 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.58.2 127.0.0.1 ::1]
I0808 12:23:10.434162   31277 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0808 12:23:10.543205   31277 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0808 12:23:10.641213   31277 kubeadm.go:309] [certs] Generating "sa" key and public key
I0808 12:23:10.641273   31277 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0808 12:23:10.894933   31277 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0808 12:23:11.466971   31277 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0808 12:23:11.938726   31277 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0808 12:23:12.372588   31277 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0808 12:23:12.769168   31277 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0808 12:23:12.770831   31277 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0808 12:23:12.775592   31277 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0808 12:23:12.788019   31277 out.go:204]     ‚ñ™ Booting up control plane ...
I0808 12:23:12.788177   31277 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0808 12:23:12.788251   31277 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0808 12:23:12.788312   31277 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0808 12:23:12.832768   31277 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0808 12:23:12.834045   31277 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0808 12:23:12.834093   31277 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0808 12:23:13.088066   31277 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0808 12:23:13.088184   31277 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0808 12:23:14.600297   31277 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 1.504300662s
I0808 12:23:14.600522   31277 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0808 12:23:28.612820   31277 kubeadm.go:309] [api-check] The API server is healthy after 14.00198368s
I0808 12:23:28.643999   31277 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0808 12:23:28.822057   31277 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0808 12:23:29.039968   31277 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0808 12:23:29.040146   31277 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0808 12:23:29.136786   31277 kubeadm.go:309] [bootstrap-token] Using token: ubsgyn.pgtnu89s51xgwbdo
I0808 12:23:29.176085   31277 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0808 12:23:29.176234   31277 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0808 12:23:29.246605   31277 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0808 12:23:29.419397   31277 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0808 12:23:29.436163   31277 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0808 12:23:29.451497   31277 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0808 12:23:29.458466   31277 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0808 12:23:29.501399   31277 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0808 12:23:30.082951   31277 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0808 12:23:30.171496   31277 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0808 12:23:30.171509   31277 kubeadm.go:309] 
I0808 12:23:30.171565   31277 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0808 12:23:30.171569   31277 kubeadm.go:309] 
I0808 12:23:30.171641   31277 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0808 12:23:30.171644   31277 kubeadm.go:309] 
I0808 12:23:30.171668   31277 kubeadm.go:309]   mkdir -p $HOME/.kube
I0808 12:23:30.171765   31277 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0808 12:23:30.171813   31277 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0808 12:23:30.171816   31277 kubeadm.go:309] 
I0808 12:23:30.171866   31277 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0808 12:23:30.171869   31277 kubeadm.go:309] 
I0808 12:23:30.171912   31277 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0808 12:23:30.171915   31277 kubeadm.go:309] 
I0808 12:23:30.171963   31277 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0808 12:23:30.172033   31277 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0808 12:23:30.172096   31277 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0808 12:23:30.172099   31277 kubeadm.go:309] 
I0808 12:23:30.172178   31277 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0808 12:23:30.172250   31277 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0808 12:23:30.172253   31277 kubeadm.go:309] 
I0808 12:23:30.172334   31277 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token ubsgyn.pgtnu89s51xgwbdo \
I0808 12:23:30.172431   31277 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:cd1de8344af9135fc9a56320b6930e1b5aba2682e28a5a9aaa25a2d163469104 \
I0808 12:23:30.172450   31277 kubeadm.go:309] 	--control-plane 
I0808 12:23:30.172453   31277 kubeadm.go:309] 
I0808 12:23:30.172532   31277 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0808 12:23:30.172535   31277 kubeadm.go:309] 
I0808 12:23:30.172611   31277 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token ubsgyn.pgtnu89s51xgwbdo \
I0808 12:23:30.172707   31277 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:cd1de8344af9135fc9a56320b6930e1b5aba2682e28a5a9aaa25a2d163469104 
I0808 12:23:30.183959   31277 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0808 12:23:30.184166   31277 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0808 12:23:30.184244   31277 cni.go:84] Creating CNI manager for ""
I0808 12:23:30.184263   31277 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0808 12:23:30.189085   31277 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0808 12:23:30.222764   31277 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0808 12:23:30.321230   31277 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0808 12:23:30.414999   31277 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0808 12:23:30.415272   31277 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0808 12:23:30.417555   31277 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_08_08T12_23_30_0700 minikube.k8s.io/version=v1.33.1 minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0808 12:23:31.525275   31277 ssh_runner.go:235] Completed: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": (1.110252702s)
I0808 12:23:31.525301   31277 ops.go:34] apiserver oom_adj: -16
I0808 12:23:31.525319   31277 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (1.110035719s)
I0808 12:23:31.525330   31277 kubeadm.go:1107] duration metric: took 1.110128641s to wait for elevateKubeSystemPrivileges
I0808 12:23:31.525361   31277 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_08_08T12_23_30_0700 minikube.k8s.io/version=v1.33.1 minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (1.107780137s)
W0808 12:23:31.525381   31277 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0808 12:23:31.525386   31277 kubeadm.go:393] duration metric: took 26.399563484s to StartCluster
I0808 12:23:31.525405   31277 settings.go:142] acquiring lock: {Name:mk19004591210340446308469f521c5cfa3e1599 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:31.525513   31277 settings.go:150] Updating kubeconfig:  /root/.kube/config
I0808 12:23:31.526274   31277 lock.go:35] WriteFile acquiring /root/.kube/config: {Name:mk72a1487fd2da23da9e8181e16f352a6105bd56 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0808 12:23:31.526544   31277 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0808 12:23:31.532819   31277 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0808 12:23:31.575443   31277 out.go:177] üîé  Verifying Kubernetes components...
I0808 12:23:31.639142   31277 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0808 12:23:31.575265   31277 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0808 12:23:31.639249   31277 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0808 12:23:31.576414   31277 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0808 12:23:31.639414   31277 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0808 12:23:31.654860   31277 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0808 12:23:31.654914   31277 host.go:66] Checking if "minikube" exists ...
I0808 12:23:31.670691   31277 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0808 12:23:31.673104   31277 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0808 12:23:31.693957   31277 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0808 12:23:32.034827   31277 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0808 12:23:32.041650   31277 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0808 12:23:32.041667   31277 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0808 12:23:32.041759   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:23:32.058663   31277 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.58.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0808 12:23:32.086942   31277 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0808 12:23:32.087046   31277 host.go:66] Checking if "minikube" exists ...
I0808 12:23:32.088329   31277 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0808 12:23:32.130977   31277 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0808 12:23:32.169777   31277 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60569 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0808 12:23:32.290296   31277 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0808 12:23:32.290308   31277 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0808 12:23:32.290372   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0808 12:23:32.401240   31277 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60569 SSHKeyPath:/root/.minikube/machines/minikube/id_rsa Username:docker}
I0808 12:23:32.424644   31277 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0808 12:23:32.636192   31277 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0808 12:23:32.737346   31277 start.go:946] {"host.minikube.internal": 192.168.58.1} host record injected into CoreDNS's ConfigMap
I0808 12:23:32.739029   31277 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0808 12:23:32.866355   31277 api_server.go:52] waiting for apiserver process to appear ...
I0808 12:23:32.866462   31277 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0808 12:23:33.128653   31277 api_server.go:72] duration metric: took 1.595713995s to wait for apiserver process to appear ...
I0808 12:23:33.128668   31277 api_server.go:88] waiting for apiserver healthz status ...
I0808 12:23:33.128692   31277 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60573/healthz ...
I0808 12:23:33.137089   31277 api_server.go:279] https://127.0.0.1:60573/healthz returned 200:
ok
I0808 12:23:33.148441   31277 api_server.go:141] control plane version: v1.30.0
I0808 12:23:33.150899   31277 api_server.go:131] duration metric: took 22.21733ms to wait for apiserver health ...
I0808 12:23:33.150944   31277 system_pods.go:43] waiting for kube-system pods to appear ...
I0808 12:23:33.150963   31277 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0808 12:23:33.156251   31277 addons.go:505] duration metric: took 1.62250099s for enable addons: enabled=[storage-provisioner default-storageclass]
I0808 12:23:33.164460   31277 system_pods.go:59] 5 kube-system pods found
I0808 12:23:33.164484   31277 system_pods.go:61] "etcd-minikube" [295336bd-7611-41ce-be65-3a9f80b63a71] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0808 12:23:33.164529   31277 system_pods.go:61] "kube-apiserver-minikube" [2d6de502-8be4-4bd9-81a1-a125140f5016] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0808 12:23:33.164556   31277 system_pods.go:61] "kube-controller-manager-minikube" [9842f5ac-734c-455a-ab1f-3a66b36d676b] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0808 12:23:33.164561   31277 system_pods.go:61] "kube-scheduler-minikube" [3b753f9e-d5ab-4986-8ec5-95f0a8bd6c58] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0808 12:23:33.164565   31277 system_pods.go:61] "storage-provisioner" [96164668-53f7-4a59-843b-db7fae5f0878] Pending
I0808 12:23:33.164584   31277 system_pods.go:74] duration metric: took 13.620686ms to wait for pod list to return data ...
I0808 12:23:33.164594   31277 kubeadm.go:576] duration metric: took 1.631657415s to wait for: map[apiserver:true system_pods:true]
I0808 12:23:33.164607   31277 node_conditions.go:102] verifying NodePressure condition ...
I0808 12:23:33.170476   31277 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0808 12:23:33.170511   31277 node_conditions.go:123] node cpu capacity is 4
I0808 12:23:33.171284   31277 node_conditions.go:105] duration metric: took 6.668489ms to run NodePressure ...
I0808 12:23:33.171302   31277 start.go:240] waiting for startup goroutines ...
I0808 12:23:33.242316   31277 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0808 12:23:33.242367   31277 start.go:245] waiting for cluster config update ...
I0808 12:23:33.242379   31277 start.go:254] writing updated cluster config ...
I0808 12:23:33.242625   31277 ssh_runner.go:195] Run: rm -f paused
I0808 12:23:33.739008   31277 start.go:600] kubectl: 1.29.2, cluster: 1.30.0 (minor skew: 1)
I0808 12:23:33.758388   31277 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 09 07:06:40 minikube dockerd[1188]: time="2024-08-09T07:06:40.469961640Z" level=info msg="ignoring event" container=b92d40ad074ab1786cf5620736a3a965949f984959ea162f1e12005f8514b709 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:12:09 minikube dockerd[1188]: time="2024-08-09T07:12:09.992907152Z" level=info msg="ignoring event" container=7f12f4e3debb75c7eba46594f39ee9580e3f71b8c432a9b65cff8f1523411b69 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:17:36 minikube dockerd[1188]: time="2024-08-09T07:17:36.437777867Z" level=info msg="ignoring event" container=a5c757a17f85a1eb67565c2c9b2add2361ffdc9a20e01209fa1b7c9ff1cc651b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:22:56 minikube dockerd[1188]: time="2024-08-09T07:22:56.721261615Z" level=info msg="ignoring event" container=ff8f95e141d5ae662ccc6e81f0d775a93e57139163b53901033a18b4b132191b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:28:19 minikube dockerd[1188]: time="2024-08-09T07:28:19.955697299Z" level=info msg="ignoring event" container=eceeeb9bab5fb75899afd0da93e48e769601e66efc505cb69d476af6b0ee4935 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:33:44 minikube dockerd[1188]: time="2024-08-09T07:33:44.758405115Z" level=info msg="ignoring event" container=548f6bf80d40596fc33347ed9950296c21ae05639ff8700e7d3c0cc5a7f478d0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:39:06 minikube dockerd[1188]: time="2024-08-09T07:39:06.876439851Z" level=info msg="ignoring event" container=036ebfb28d20bf4b1e28a02b00c305a82c88d59ed5f223efba0d48c44730303a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:44:34 minikube dockerd[1188]: time="2024-08-09T07:44:34.257360593Z" level=info msg="ignoring event" container=fb122483a520f68e92302fb2b35fa915344f328db7551b82d2cbe4621f16328b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:50:03 minikube dockerd[1188]: time="2024-08-09T07:50:03.331565972Z" level=info msg="ignoring event" container=4da3296b3c1aa643e4136a3467eab9947ee732b9d1aa5ba8a210c613d80f7adb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 07:55:30 minikube dockerd[1188]: time="2024-08-09T07:55:30.159926844Z" level=info msg="ignoring event" container=2c52f380c8a451e59eacd48d4c273819bfe134cf3894b3a05aaaf4b6f1af280c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:00:57 minikube dockerd[1188]: time="2024-08-09T08:00:57.965805857Z" level=info msg="ignoring event" container=4c93509cd419c2c1e30593bc8af525bb85b8032cffb08b40a1266d1332d29252 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:06:25 minikube dockerd[1188]: time="2024-08-09T08:06:25.913222344Z" level=info msg="ignoring event" container=0025f252ddc37ec0a69de850369b9bf5f1b0bf34e207a60e8ddd092ce76d783f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:11:49 minikube dockerd[1188]: time="2024-08-09T08:11:49.042321626Z" level=info msg="ignoring event" container=0e666ed703fb0f5de83fd21d6c533a53847826f7dff2abe050d37ba5b0a9f20d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:17:13 minikube dockerd[1188]: time="2024-08-09T08:17:13.572989700Z" level=info msg="ignoring event" container=2398ddc850b961eb2c6b494bc87bea5778c35b956970970e7d7dbfe9b53de3c0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:22:38 minikube dockerd[1188]: time="2024-08-09T08:22:38.192833285Z" level=info msg="ignoring event" container=5a42c1c294041a6031207160c416e82d21f51aaf75ed0dbc4937be45460cb926 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:28:03 minikube dockerd[1188]: time="2024-08-09T08:28:03.379617219Z" level=info msg="ignoring event" container=a0c5be2a32c4492077212df6d6f4df6db6d37f183a2fccfc955a921cacb330d8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:33:27 minikube dockerd[1188]: time="2024-08-09T08:33:27.499899918Z" level=info msg="ignoring event" container=cbc29d87aafa6b21569205d6af33a5c7fa37ed91b0659309c610c4c4ec2ca515 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:38:57 minikube dockerd[1188]: time="2024-08-09T08:38:57.509832856Z" level=info msg="ignoring event" container=2084bf6be28e26b5888f0d4f0a414fe3ec06783f79dc74f9d168127a1cfa5496 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:44:16 minikube dockerd[1188]: time="2024-08-09T08:44:16.489487888Z" level=info msg="ignoring event" container=013c28e34eb478e94138f659471c16b732b6bbdaf99ce2545ed19ae2b266a4bd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:49:36 minikube dockerd[1188]: time="2024-08-09T08:49:36.952281052Z" level=info msg="ignoring event" container=588ac4d84160124718603fa2ccaacb816e5d7bb2001fce22fc774debeed5577c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 08:54:58 minikube dockerd[1188]: time="2024-08-09T08:54:58.645381009Z" level=info msg="ignoring event" container=2600a47ecd54407d98aa28d69d9c989b0fffa3f9f77e596d7aba539d87f73d5f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:00:23 minikube dockerd[1188]: time="2024-08-09T09:00:23.230812728Z" level=info msg="ignoring event" container=42fd7aec7e523854b25dab91d89fd29a2f1992a3dddb5698cb93d384dfefaca4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:05:42 minikube dockerd[1188]: time="2024-08-09T09:05:42.590862247Z" level=info msg="ignoring event" container=30e34a6d1988fc23b73d1a5f773c7c1b11cafabf0888a709a961a09dacc5e5f3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:11:09 minikube dockerd[1188]: time="2024-08-09T09:11:09.237483492Z" level=info msg="ignoring event" container=963b179a4cf6d04638d7bce39828fddcd49b3857fdfec35247894831df3beeb2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:16:35 minikube dockerd[1188]: time="2024-08-09T09:16:35.628408770Z" level=info msg="ignoring event" container=048e339d682f536fb460539c45f9f4b1b8589bb89887b3af140a0e017509b3ee module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:21:55 minikube dockerd[1188]: time="2024-08-09T09:21:55.859869204Z" level=info msg="ignoring event" container=6f75f02082fa7a333d36d1eedf319e156d374f6a5483759cc7d70a6240b897bb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:27:14 minikube dockerd[1188]: time="2024-08-09T09:27:14.017392535Z" level=info msg="ignoring event" container=5d0ece560cd8bea3c8dab637f90d5ac835e44e405a68bd4966a3111a63f40832 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:32:45 minikube dockerd[1188]: time="2024-08-09T09:32:45.199710768Z" level=info msg="ignoring event" container=482f3d3ce2e58c62d5022611f721982440513517368dd90fe30b07c5155a76e7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:38:10 minikube dockerd[1188]: time="2024-08-09T09:38:10.272423750Z" level=info msg="ignoring event" container=17e38af77c13e89d390aec7921d0ec52cc61fbe19aa6b9a7bad7198d624a59b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:43:29 minikube dockerd[1188]: time="2024-08-09T09:43:29.668713129Z" level=info msg="ignoring event" container=5d862a8716ad52f99f3963add9185fb05b8a7d5b8d09795c5415246885e922fb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:48:49 minikube dockerd[1188]: time="2024-08-09T09:48:49.328590587Z" level=info msg="ignoring event" container=0128fe53ba311d79fd26a9f1399934df607ee05b29e04ffe36477580639f63ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:54:16 minikube dockerd[1188]: time="2024-08-09T09:54:16.757484857Z" level=info msg="ignoring event" container=44d1b014764c244e2e1d2e3185fe94d5e7655c874b4b4c15fdcbe0b7f19f2718 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 09:59:34 minikube dockerd[1188]: time="2024-08-09T09:59:34.799353764Z" level=info msg="ignoring event" container=5449cef734bc882301ed0163fcee45b984fbeb4a893369dd48916bc9d33fc3b1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:04:54 minikube dockerd[1188]: time="2024-08-09T10:04:54.988473028Z" level=info msg="ignoring event" container=547b132c748e1450186480eefa45c0b64603baaa24f4df6259ceda0c3e7e7840 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:10:13 minikube dockerd[1188]: time="2024-08-09T10:10:13.425872859Z" level=info msg="ignoring event" container=8c43631cb23320e1bd8a6c1b08c61b8062d452069407c4d004d1dfd185260c9c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:15:36 minikube dockerd[1188]: time="2024-08-09T10:15:36.661362193Z" level=info msg="ignoring event" container=fc95ca13804770125b4a093df2e947ce9d95d3757bac286af1e4e6962a5d816c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:21:04 minikube dockerd[1188]: time="2024-08-09T10:21:04.786999150Z" level=info msg="ignoring event" container=8ca8425f40b7adefb612f3d4ef48b427ff8f544be7a66e2428ca0c5651c50934 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:26:31 minikube dockerd[1188]: time="2024-08-09T10:26:31.791389028Z" level=info msg="ignoring event" container=cd059c81e118d0e118156384b32ad7b625c5abed55a5bd5acd87e9ff3efcc0c0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:31:51 minikube dockerd[1188]: time="2024-08-09T10:31:51.324773381Z" level=info msg="ignoring event" container=40d998b759210204fda66336d5a8bed5e75cd16951a68a9d4b0acb18e473393a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:37:10 minikube dockerd[1188]: time="2024-08-09T10:37:10.930091684Z" level=info msg="ignoring event" container=43b47cfda7e5a95636b8e43425aaa063b798888065abe4d706df548df961f180 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:42:39 minikube dockerd[1188]: time="2024-08-09T10:42:39.156441603Z" level=info msg="ignoring event" container=511b4647feca28b04cd5f8619b48cc0c9ccc40fa98f287101e1d7d4c4f2c5f89 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:48:03 minikube dockerd[1188]: time="2024-08-09T10:48:03.753459796Z" level=info msg="ignoring event" container=9c52ee3b0a40ba73a25fe750dab762ec92de561276e324edee45b242d8969ba1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:53:23 minikube dockerd[1188]: time="2024-08-09T10:53:23.866392627Z" level=info msg="ignoring event" container=1fef5e87ab24edd121fd8553d029504f49b0785a956188cb2e3e73bc9af9dad4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 10:58:43 minikube dockerd[1188]: time="2024-08-09T10:58:43.596146211Z" level=info msg="ignoring event" container=274a5d474f92852fb39ee74338adf13cc9e5946e5c79d7227a2b307cb73b0cb4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:04:04 minikube dockerd[1188]: time="2024-08-09T11:04:04.186995627Z" level=info msg="ignoring event" container=f99fd940a3e8c3960c9e4c73a0db209e03222c15f183096c8d99f3c2b339f5c4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:09:24 minikube dockerd[1188]: time="2024-08-09T11:09:24.026100697Z" level=info msg="ignoring event" container=57e09b9b6fbb4f9fb8065d7aac50414d54508b2ec66a5cd028682eeff586f3c9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:14:48 minikube dockerd[1188]: time="2024-08-09T11:14:48.240337922Z" level=info msg="ignoring event" container=b345945798185b94eb0a2d68e071071d28d6fed752a657dc298ebd6e5db3fc02 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:20:13 minikube dockerd[1188]: time="2024-08-09T11:20:13.845175293Z" level=info msg="ignoring event" container=237ab67538af6a9fad764d4bc456e489028aec71454da3b4e36cfdc65d93942d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:25:40 minikube dockerd[1188]: time="2024-08-09T11:25:40.058916157Z" level=info msg="ignoring event" container=7f27e776fff29aa4f00df042f599742959982156cadd19d8713c00f67fb15ce9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:31:11 minikube dockerd[1188]: time="2024-08-09T11:31:11.203479858Z" level=info msg="ignoring event" container=9948438b401e7327ac343967256effcfe1bbebc8aa8b0db542cfef4b5070bdb1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:36:29 minikube dockerd[1188]: time="2024-08-09T11:36:29.701989587Z" level=info msg="ignoring event" container=f219f7a05ef7d73183cac988fa82894aeef431f71fae083ce336297c37f1c578 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:41:53 minikube dockerd[1188]: time="2024-08-09T11:41:53.423626547Z" level=info msg="ignoring event" container=a4b198e5d416bf26b4434b0da6eaf0345b06e42e1e6f3dd23644b7b3a5117df4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:47:25 minikube dockerd[1188]: time="2024-08-09T11:47:25.026666642Z" level=info msg="ignoring event" container=50ac890e94107cd915b291451fffa3fa84fc6104a5c8f2ccf44efdcfcec9a4aa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:52:45 minikube dockerd[1188]: time="2024-08-09T11:52:45.409311550Z" level=info msg="ignoring event" container=cd9a2c5981e77170473fef7778545e01a95bdb086cf043041fae57d982c7d860 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:58:24 minikube dockerd[1188]: time="2024-08-09T11:58:24.725008579Z" level=info msg="ignoring event" container=2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 11:58:32 minikube cri-dockerd[1410]: time="2024-08-09T11:58:32Z" level=error msg="error getting RW layer size for container ID 'cd9a2c5981e77170473fef7778545e01a95bdb086cf043041fae57d982c7d860': Error response from daemon: No such container: cd9a2c5981e77170473fef7778545e01a95bdb086cf043041fae57d982c7d860"
Aug 09 11:58:32 minikube cri-dockerd[1410]: time="2024-08-09T11:58:32Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'cd9a2c5981e77170473fef7778545e01a95bdb086cf043041fae57d982c7d860'"
Aug 09 12:03:48 minikube dockerd[1188]: time="2024-08-09T12:03:48.113262014Z" level=info msg="ignoring event" container=d07abcfa184a91c717243fff70be46627cb3eb701d46a6b96587a0a88eb74692 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 09 12:04:03 minikube cri-dockerd[1410]: time="2024-08-09T12:04:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ff01392e996743dda5b3c8ced9011272d0f2d89ec22b82692bc27518c0d35633/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 09 12:04:04 minikube dockerd[1188]: time="2024-08-09T12:04:04.693311503Z" level=info msg="ignoring event" container=f6f321cc2db800b30e320f40937f1f672551ceb30bfbab1b9fefc8de7d7f3e3c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                              CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
f6ee741ed2796       7ac3e08c24126                                                                      7 minutes ago       Running             pg-admin                  0                   ff01392e99674       pg-admin-deployment-5d46bd97cd-sn22v
8e424d689d994       postgres@sha256:02547253a07e6edd0c070caba1d2a019b7dc7df98b948dc9a909e1808eb77024   17 hours ago        Running             postgres                  0                   0b1a70a269761       postgres-deployment-66b6655f68-q778x
889a73a873620       6e38f40d628db                                                                      21 hours ago        Running             storage-provisioner       1                   b223c55e15fea       storage-provisioner
451c078a4cca5       cbb01a7bd410d                                                                      21 hours ago        Running             coredns                   0                   2d41b971d784e       coredns-7db6d8ff4d-6dwfv
4779a4ba40e26       6e38f40d628db                                                                      21 hours ago        Exited              storage-provisioner       0                   b223c55e15fea       storage-provisioner
8e2e5b7829102       a0bf559e280cf                                                                      21 hours ago        Running             kube-proxy                0                   79e8853f1fd8f       kube-proxy-2922j
38dd7bac24a3c       c7aad43836fa5                                                                      21 hours ago        Running             kube-controller-manager   0                   f477b1260eaf4       kube-controller-manager-minikube
d3fead244694c       3861cfcd7c04c                                                                      21 hours ago        Running             etcd                      0                   86c57f30780fd       etcd-minikube
a5fca69f982db       259c8277fcbbc                                                                      21 hours ago        Running             kube-scheduler            0                   4c289d515abfe       kube-scheduler-minikube
17ba70c5448d5       c42f13656d0b2                                                                      21 hours ago        Running             kube-apiserver            0                   6ad7732f59727       kube-apiserver-minikube


==> coredns [451c078a4cca] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 75e5db48a73272e2c90919c8256e5cca0293ae0ed689e2ed44f1254a9589c3d004cb3e693d059116718c47e9305987b828b11b2735a1cefa59e4a9489dda5cee
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:60999 - 20983 "HINFO IN 7561418876062797970.3718027539717778409. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.09915413s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.436710671s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_08_08T12_23_30_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 08 Aug 2024 15:23:26 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 09 Aug 2024 12:11:39 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 09 Aug 2024 12:07:05 +0000   Thu, 08 Aug 2024 15:23:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 09 Aug 2024 12:07:05 +0000   Thu, 08 Aug 2024 15:23:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 09 Aug 2024 12:07:05 +0000   Thu, 08 Aug 2024 15:23:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 09 Aug 2024 12:07:05 +0000   Thu, 08 Aug 2024 15:23:30 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8074112Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8074112Ki
  pods:               110
System Info:
  Machine ID:                 3ba30991ad284c00bbb4e03dbaff01cf
  System UUID:                3ba30991ad284c00bbb4e03dbaff01cf
  Boot ID:                    b78a0266-84d8-4859-8718-983cb42e1987
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     pg-admin-deployment-5d46bd97cd-sn22v    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m44s
  default                     postgres-deployment-66b6655f68-q778x    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         16h
  kube-system                 coredns-7db6d8ff4d-6dwfv                100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     20h
  kube-system                 etcd-minikube                           100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         20h
  kube-system                 kube-apiserver-minikube                 250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
  kube-system                 kube-controller-manager-minikube        200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
  kube-system                 kube-proxy-2922j                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
  kube-system                 kube-scheduler-minikube                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
  kube-system                 storage-provisioner                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[Aug 8 13:41] PCI: Fatal: No config space access function found
[  +0.020224] PCI: System does not support PCI
[  +0.022011] kvm: no hardware support
[  +0.000004] kvm: no hardware support
[  +0.995884] FS-Cache: Duplicate cookie detected
[  +0.000568] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000512] FS-Cache: O-cookie d=0000000077608fb9{9P.session} n=0000000001c0e975
[  +0.001375] FS-Cache: O-key=[10] '34323934393337343032'
[  +0.002226] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000899] FS-Cache: N-cookie d=0000000077608fb9{9P.session} n=000000004aea51dd
[  +0.001336] FS-Cache: N-key=[10] '34323934393337343032'
[  +1.396228] Failed to connect to bus: No such file or directory
[  +0.582906] systemd-journald[47]: File /var/log/journal/df87d02c17b546eaa18609b0191e52e5/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +1.398360] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +8.953368] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.
[Aug 8 13:42] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001141] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.000868] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.015924] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.010571] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.005474] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.107141] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.124155] WSL (1) WARNING: /usr/share/zoneinfo/America/Buenos_Aires not found. Is the tzdata package installed?
[  +0.365238] Exception: 
[  +0.000005] Operation canceled @p9io.cpp:258 (AcceptAsync)

[ +18.078252] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.048425] WSL (1) WARNING: /usr/share/zoneinfo/America/Buenos_Aires not found. Is the tzdata package installed?
[  +0.151037] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.447529] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.026253] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.023335] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.014188] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.054323] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.146813] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.158447] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.159362] WSL (1) WARNING: /usr/share/zoneinfo/America/Buenos_Aires not found. Is the tzdata package installed?
[  +1.327781] new mount options do not match the existing superblock, will be ignored
[  +0.000219] netlink: 'init': attribute type 4 has an invalid length.
[Aug 8 14:32] hrtimer: interrupt took 12745510 ns


==> etcd [d3fead244694] <==
{"level":"warn","ts":"2024-08-09T12:07:54.300808Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238529253512659069,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-08-09T12:07:54.490643Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.238899491s","expected-duration":"1s"}
{"level":"info","ts":"2024-08-09T12:07:54.490886Z","caller":"traceutil/trace.go:171","msg":"trace[35987867] linearizableReadLoop","detail":"{readStateIndex:77639; appliedIndex:77638; }","duration":"1.192231221s","start":"2024-08-09T12:07:53.29864Z","end":"2024-08-09T12:07:54.490871Z","steps":["trace[35987867] 'read index received'  (duration: 1.192224121s)","trace[35987867] 'applied index is now lower than readState.Index'  (duration: 5.9¬µs)"],"step_count":2}
{"level":"info","ts":"2024-08-09T12:07:54.490889Z","caller":"traceutil/trace.go:171","msg":"trace[1799342798] transaction","detail":"{read_only:false; response_revision:62072; number_of_response:1; }","duration":"1.239244388s","start":"2024-08-09T12:07:53.251634Z","end":"2024-08-09T12:07:54.490878Z","steps":["trace[1799342798] 'process raft request'  (duration: 1.239128489s)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:07:54.491051Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.192419318s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:604"}
{"level":"warn","ts":"2024-08-09T12:07:54.491075Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:07:53.251618Z","time spent":"1.239405687s","remote":"127.0.0.1:40784","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:62064 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-08-09T12:07:54.491085Z","caller":"traceutil/trace.go:171","msg":"trace[1625086762] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:62072; }","duration":"1.192482918s","start":"2024-08-09T12:07:53.298593Z","end":"2024-08-09T12:07:54.491076Z","steps":["trace[1625086762] 'agreement among raft nodes before linearized reading'  (duration: 1.192334919s)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:07:54.491108Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:07:53.298582Z","time spent":"1.192519418s","remote":"127.0.0.1:40712","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":627,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-08-09T12:07:54.528572Z","caller":"traceutil/trace.go:171","msg":"trace[264877592] transaction","detail":"{read_only:false; response_revision:62073; number_of_response:1; }","duration":"225.518523ms","start":"2024-08-09T12:07:54.303034Z","end":"2024-08-09T12:07:54.528553Z","steps":["trace[264877592] 'process raft request'  (duration: 209.274972ms)","trace[264877592] 'compare'  (duration: 16.096252ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-09T12:07:54.528614Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"563.124914ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.58.2\" ","response":"range_response_count:1 size:135"}
{"level":"info","ts":"2024-08-09T12:07:54.528648Z","caller":"traceutil/trace.go:171","msg":"trace[1542877920] range","detail":"{range_begin:/registry/masterleases/192.168.58.2; range_end:; response_count:1; response_revision:62074; }","duration":"563.188213ms","start":"2024-08-09T12:07:53.965452Z","end":"2024-08-09T12:07:54.52864Z","steps":["trace[1542877920] 'agreement among raft nodes before linearized reading'  (duration: 563.121714ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:07:54.528672Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:07:53.96544Z","time spent":"563.225013ms","remote":"127.0.0.1:40596","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":158,"request content":"key:\"/registry/masterleases/192.168.58.2\" "}
{"level":"warn","ts":"2024-08-09T12:07:54.528791Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"499.5045ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-09T12:07:54.528811Z","caller":"traceutil/trace.go:171","msg":"trace[676694285] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:62074; }","duration":"499.5437ms","start":"2024-08-09T12:07:54.029261Z","end":"2024-08-09T12:07:54.528805Z","steps":["trace[676694285] 'agreement among raft nodes before linearized reading'  (duration: 499.5159ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:07:54.528827Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:07:54.029235Z","time spent":"499.588099ms","remote":"127.0.0.1:40586","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-08-09T12:08:42.17441Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":61831}
{"level":"info","ts":"2024-08-09T12:08:42.186543Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":61831,"took":"11.858238ms","hash":897631095,"current-db-size-bytes":1740800,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":1212416,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-08-09T12:08:42.186606Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":897631095,"revision":61831,"compact-revision":61588}
{"level":"warn","ts":"2024-08-09T12:09:54.221971Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238529253512659554,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-08-09T12:09:54.722206Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238529253512659554,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-08-09T12:09:54.776951Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.482057737s","expected-duration":"1s"}
{"level":"info","ts":"2024-08-09T12:09:54.77722Z","caller":"traceutil/trace.go:171","msg":"trace[188022881] linearizableReadLoop","detail":"{readStateIndex:77760; appliedIndex:77759; }","duration":"1.055903794s","start":"2024-08-09T12:09:53.721302Z","end":"2024-08-09T12:09:54.777206Z","steps":["trace[188022881] 'read index received'  (duration: 1.055760593s)","trace[188022881] 'applied index is now lower than readState.Index'  (duration: 142.301¬µs)"],"step_count":2}
{"level":"info","ts":"2024-08-09T12:09:54.77727Z","caller":"traceutil/trace.go:171","msg":"trace[1296548280] transaction","detail":"{read_only:false; response_revision:62168; number_of_response:1; }","duration":"1.482437644s","start":"2024-08-09T12:09:53.294818Z","end":"2024-08-09T12:09:54.777256Z","steps":["trace[1296548280] 'process raft request'  (duration: 1.482286742s)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:09:54.777308Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.055987896s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-09T12:09:54.777333Z","caller":"traceutil/trace.go:171","msg":"trace[1798888653] range","detail":"{range_begin:/registry/certificatesigningrequests/; range_end:/registry/certificatesigningrequests0; response_count:0; response_revision:62168; }","duration":"1.056026397s","start":"2024-08-09T12:09:53.721298Z","end":"2024-08-09T12:09:54.777324Z","steps":["trace[1798888653] 'agreement among raft nodes before linearized reading'  (duration: 1.055974896s)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:09:54.777341Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:09:53.294803Z","time spent":"1.482498545s","remote":"127.0.0.1:40712","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:62167 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-08-09T12:09:54.777353Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:09:53.721248Z","time spent":"1.056099297s","remote":"127.0.0.1:40774","response type":"/etcdserverpb.KV/Range","request count":0,"request size":80,"response count":0,"response size":29,"request content":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true "}
{"level":"warn","ts":"2024-08-09T12:09:54.777439Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"911.000394ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/mutatingwebhookconfigurations/\" range_end:\"/registry/mutatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-09T12:09:54.77746Z","caller":"traceutil/trace.go:171","msg":"trace[314228923] range","detail":"{range_begin:/registry/mutatingwebhookconfigurations/; range_end:/registry/mutatingwebhookconfigurations0; response_count:0; response_revision:62168; }","duration":"911.043394ms","start":"2024-08-09T12:09:53.86641Z","end":"2024-08-09T12:09:54.777454Z","steps":["trace[314228923] 'agreement among raft nodes before linearized reading'  (duration: 911.008894ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:09:54.777468Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"752.995039ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-09T12:09:54.777503Z","caller":"traceutil/trace.go:171","msg":"trace[1134486178] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:62168; }","duration":"753.05204ms","start":"2024-08-09T12:09:54.024443Z","end":"2024-08-09T12:09:54.777495Z","steps":["trace[1134486178] 'agreement among raft nodes before linearized reading'  (duration: 752.998239ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:09:54.77751Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"808.774094ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.58.2\" ","response":"range_response_count:1 size:135"}
{"level":"warn","ts":"2024-08-09T12:09:54.777526Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:09:54.02443Z","time spent":"753.09074ms","remote":"127.0.0.1:40586","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-08-09T12:09:54.777532Z","caller":"traceutil/trace.go:171","msg":"trace[1230164465] range","detail":"{range_begin:/registry/masterleases/192.168.58.2; range_end:; response_count:1; response_revision:62168; }","duration":"808.820294ms","start":"2024-08-09T12:09:53.968707Z","end":"2024-08-09T12:09:54.777528Z","steps":["trace[1230164465] 'agreement among raft nodes before linearized reading'  (duration: 808.781894ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:09:54.777548Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:09:53.968694Z","time spent":"808.850494ms","remote":"127.0.0.1:40596","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":158,"request content":"key:\"/registry/masterleases/192.168.58.2\" "}
{"level":"warn","ts":"2024-08-09T12:09:54.777595Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"691.75612ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-08-09T12:09:54.777478Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:09:53.866391Z","time spent":"911.082794ms","remote":"127.0.0.1:41044","response type":"/etcdserverpb.KV/Range","request count":0,"request size":86,"response count":0,"response size":29,"request content":"key:\"/registry/mutatingwebhookconfigurations/\" range_end:\"/registry/mutatingwebhookconfigurations0\" count_only:true "}
{"level":"info","ts":"2024-08-09T12:09:54.777617Z","caller":"traceutil/trace.go:171","msg":"trace[1471244758] range","detail":"{range_begin:/registry/csidrivers/; range_end:/registry/csidrivers0; response_count:0; response_revision:62168; }","duration":"691.807921ms","start":"2024-08-09T12:09:54.085803Z","end":"2024-08-09T12:09:54.777611Z","steps":["trace[1471244758] 'agreement among raft nodes before linearized reading'  (duration: 691.772121ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:09:54.777696Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:09:54.08579Z","time spent":"691.898822ms","remote":"127.0.0.1:40946","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":0,"response size":29,"request content":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true "}
{"level":"warn","ts":"2024-08-09T12:10:03.46303Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238529253512659589,"retry-timeout":"500ms"}
{"level":"info","ts":"2024-08-09T12:10:03.555101Z","caller":"traceutil/trace.go:171","msg":"trace[1266520182] linearizableReadLoop","detail":"{readStateIndex:77769; appliedIndex:77768; }","duration":"592.460475ms","start":"2024-08-09T12:10:02.962627Z","end":"2024-08-09T12:10:03.555087Z","steps":["trace[1266520182] 'read index received'  (duration: 592.296762ms)","trace[1266520182] 'applied index is now lower than readState.Index'  (duration: 163.113¬µs)"],"step_count":2}
{"level":"info","ts":"2024-08-09T12:10:03.555178Z","caller":"traceutil/trace.go:171","msg":"trace[1863241231] transaction","detail":"{read_only:false; response_revision:62175; number_of_response:1; }","duration":"735.000604ms","start":"2024-08-09T12:10:02.820166Z","end":"2024-08-09T12:10:03.555166Z","steps":["trace[1863241231] 'process raft request'  (duration: 734.803089ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:10:03.555222Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"592.586385ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-08-09T12:10:03.555253Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:10:02.820147Z","time spent":"735.05841ms","remote":"127.0.0.1:40712","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:62174 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-08-09T12:10:03.555247Z","caller":"traceutil/trace.go:171","msg":"trace[1200706885] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:62175; }","duration":"592.640189ms","start":"2024-08-09T12:10:02.9626Z","end":"2024-08-09T12:10:03.555241Z","steps":["trace[1200706885] 'agreement among raft nodes before linearized reading'  (duration: 592.592886ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:10:03.555288Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:10:02.962588Z","time spent":"592.693193ms","remote":"127.0.0.1:40586","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-08-09T12:10:03.555328Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"532.514053ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-09T12:10:03.555355Z","caller":"traceutil/trace.go:171","msg":"trace[809330940] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:62175; }","duration":"532.563657ms","start":"2024-08-09T12:10:03.022784Z","end":"2024-08-09T12:10:03.555348Z","steps":["trace[809330940] 'agreement among raft nodes before linearized reading'  (duration: 532.523854ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:10:03.555374Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:10:03.022772Z","time spent":"532.597959ms","remote":"127.0.0.1:40582","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-08-09T12:10:56.521771Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238529253512659793,"retry-timeout":"500ms"}
{"level":"info","ts":"2024-08-09T12:10:56.654291Z","caller":"traceutil/trace.go:171","msg":"trace[2100870819] linearizableReadLoop","detail":"{readStateIndex:77822; appliedIndex:77821; }","duration":"633.178515ms","start":"2024-08-09T12:10:56.02109Z","end":"2024-08-09T12:10:56.654269Z","steps":["trace[2100870819] 'read index received'  (duration: 632.979808ms)","trace[2100870819] 'applied index is now lower than readState.Index'  (duration: 197.707¬µs)"],"step_count":2}
{"level":"info","ts":"2024-08-09T12:10:56.654341Z","caller":"traceutil/trace.go:171","msg":"trace[923764855] transaction","detail":"{read_only:false; response_revision:62217; number_of_response:1; }","duration":"707.932437ms","start":"2024-08-09T12:10:55.946396Z","end":"2024-08-09T12:10:56.654328Z","steps":["trace[923764855] 'process raft request'  (duration: 707.767431ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:10:56.654382Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"633.296819ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-09T12:10:56.654404Z","caller":"traceutil/trace.go:171","msg":"trace[2053521970] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:62217; }","duration":"633.352121ms","start":"2024-08-09T12:10:56.021046Z","end":"2024-08-09T12:10:56.654398Z","steps":["trace[2053521970] 'agreement among raft nodes before linearized reading'  (duration: 633.303919ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:10:56.654407Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:10:55.946381Z","time spent":"707.988739ms","remote":"127.0.0.1:40712","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:62215 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-08-09T12:10:56.654422Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:10:56.021035Z","time spent":"633.383421ms","remote":"127.0.0.1:40586","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-08-09T12:10:56.654452Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"205.136446ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-09T12:10:56.654481Z","caller":"traceutil/trace.go:171","msg":"trace[516054188] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:62217; }","duration":"205.189548ms","start":"2024-08-09T12:10:56.449284Z","end":"2024-08-09T12:10:56.654473Z","steps":["trace[516054188] 'agreement among raft nodes before linearized reading'  (duration: 205.142347ms)"],"step_count":1}
{"level":"info","ts":"2024-08-09T12:11:21.465537Z","caller":"traceutil/trace.go:171","msg":"trace[1187603384] transaction","detail":"{read_only:false; response_revision:62237; number_of_response:1; }","duration":"677.402629ms","start":"2024-08-09T12:11:20.788118Z","end":"2024-08-09T12:11:21.465521Z","steps":["trace[1187603384] 'process raft request'  (duration: 677.258922ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-09T12:11:21.465653Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-09T12:11:20.788098Z","time spent":"677.508334ms","remote":"127.0.0.1:40712","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:62235 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}


==> kernel <==
 12:11:47 up 22:30,  0 users,  load average: 1.58, 0.83, 0.69
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [17ba70c5448d] <==
Trace[1313920038]: ["GuaranteedUpdate etcd3" audit-id:b9c81b40-4986-41a8-b029-f6844d59e7d8,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 793ms (12:00:32.004)
Trace[1313920038]:  ---"Txn call completed" 792ms (12:00:32.797)]
Trace[1313920038]: [793.121637ms] [793.121637ms] END
I0809 12:01:07.523909       1 trace.go:236] Trace[1018613733]: "Update" accept:application/json, */*,audit-id:9b035355-e6a8-47de-b6a6-b0987813a27b,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:01:06.988) (total time: 535ms):
Trace[1018613733]: ["GuaranteedUpdate etcd3" audit-id:9b035355-e6a8-47de-b6a6-b0987813a27b,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 535ms (12:01:06.988)
Trace[1018613733]:  ---"Txn call completed" 534ms (12:01:07.523)]
Trace[1018613733]: [535.397555ms] [535.397555ms] END
I0809 12:02:12.509522       1 trace.go:236] Trace[678804160]: "Update" accept:application/json, */*,audit-id:3b351706-a1a9-4d4a-b677-19bd8b8a9e15,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:02:11.994) (total time: 515ms):
Trace[678804160]: ["GuaranteedUpdate etcd3" audit-id:3b351706-a1a9-4d4a-b677-19bd8b8a9e15,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 514ms (12:02:11.994)
Trace[678804160]:  ---"Txn call completed" 514ms (12:02:12.509)]
Trace[678804160]: [515.216466ms] [515.216466ms] END
I0809 12:02:49.643748       1 trace.go:236] Trace[405559604]: "Update" accept:application/json, */*,audit-id:bb022e33-2474-46ae-a163-1851b3176631,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:02:48.820) (total time: 823ms):
Trace[405559604]: ["GuaranteedUpdate etcd3" audit-id:bb022e33-2474-46ae-a163-1851b3176631,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 822ms (12:02:48.820)
Trace[405559604]:  ---"Txn call completed" 821ms (12:02:49.643)]
Trace[405559604]: [823.173027ms] [823.173027ms] END
I0809 12:03:31.025638       1 trace.go:236] Trace[460498224]: "Update" accept:application/json, */*,audit-id:a2f9e7ac-d739-4a85-aaa9-126fad595d5b,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:03:30.327) (total time: 698ms):
Trace[460498224]: ["GuaranteedUpdate etcd3" audit-id:a2f9e7ac-d739-4a85-aaa9-126fad595d5b,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 697ms (12:03:30.328)
Trace[460498224]:  ---"Txn call completed" 696ms (12:03:31.025)]
Trace[460498224]: [698.211458ms] [698.211458ms] END
I0809 12:04:16.163112       1 trace.go:236] Trace[162570548]: "Update" accept:application/json, */*,audit-id:3db97d9c-5500-47f4-94c8-e1bc8b739488,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:04:15.295) (total time: 867ms):
Trace[162570548]: ["GuaranteedUpdate etcd3" audit-id:3db97d9c-5500-47f4-94c8-e1bc8b739488,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 866ms (12:04:15.296)
Trace[162570548]:  ---"Txn call completed" 866ms (12:04:16.162)]
Trace[162570548]: [867.084114ms] [867.084114ms] END
I0809 12:04:50.549882       1 trace.go:236] Trace[1150217201]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3a34d491-32dc-4525-89e7-b0e4ec319a0b,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (09-Aug-2024 12:04:49.530) (total time: 1019ms):
Trace[1150217201]: ["GuaranteedUpdate etcd3" audit-id:3a34d491-32dc-4525-89e7-b0e4ec319a0b,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1019ms (12:04:49.530)
Trace[1150217201]:  ---"Txn call completed" 1018ms (12:04:50.549)]
Trace[1150217201]: [1.019485815s] [1.019485815s] END
I0809 12:06:08.605401       1 trace.go:236] Trace[6844143]: "Update" accept:application/json, */*,audit-id:64f6e0e3-0f8f-4633-a18b-5d1adbf778d0,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:06:07.322) (total time: 1282ms):
Trace[6844143]: ["GuaranteedUpdate etcd3" audit-id:64f6e0e3-0f8f-4633-a18b-5d1adbf778d0,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1282ms (12:06:07.322)
Trace[6844143]:  ---"Txn call completed" 1281ms (12:06:08.605)]
Trace[6844143]: [1.282856257s] [1.282856257s] END
I0809 12:07:54.491412       1 trace.go:236] Trace[2096629307]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:efc04ff0-7f71-4ba3-86f2-11f99f56ad1b,client:192.168.58.2,api-group:coordination.k8s.io,api-version:v1,name:minikube,subresource:,namespace:kube-node-lease,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (09-Aug-2024 12:07:53.250) (total time: 1240ms):
Trace[2096629307]: ["GuaranteedUpdate etcd3" audit-id:efc04ff0-7f71-4ba3-86f2-11f99f56ad1b,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 1240ms (12:07:53.250)
Trace[2096629307]:  ---"Txn call completed" 1240ms (12:07:54.491)]
Trace[2096629307]: [1.240988171s] [1.240988171s] END
I0809 12:07:54.491521       1 trace.go:236] Trace[1969359404]: "Get" accept:application/json, */*,audit-id:03cd3268-a3a6-4939-8b81-25c68f8fb637,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (09-Aug-2024 12:07:53.298) (total time: 1193ms):
Trace[1969359404]: ---"About to write a response" 1193ms (12:07:54.491)
Trace[1969359404]: [1.19336201s] [1.19336201s] END
I0809 12:07:54.551124       1 trace.go:236] Trace[579604550]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Aug-2024 12:07:53.964) (total time: 586ms):
Trace[579604550]: ---"initial value restored" 564ms (12:07:54.529)
Trace[579604550]: [586.121702ms] [586.121702ms] END
I0809 12:09:54.778650       1 trace.go:236] Trace[412288856]: "Update" accept:application/json, */*,audit-id:107d0a09-ef0b-4eb0-856e-df3e0292ee38,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:09:53.293) (total time: 1485ms):
Trace[412288856]: ["GuaranteedUpdate etcd3" audit-id:107d0a09-ef0b-4eb0-856e-df3e0292ee38,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1484ms (12:09:53.293)
Trace[412288856]:  ---"Txn call completed" 1484ms (12:09:54.778)]
Trace[412288856]: [1.48505512s] [1.48505512s] END
I0809 12:09:54.784479       1 trace.go:236] Trace[2104910580]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Aug-2024 12:09:53.968) (total time: 816ms):
Trace[2104910580]: ---"initial value restored" 809ms (12:09:54.778)
Trace[2104910580]: [816.240881ms] [816.240881ms] END
I0809 12:10:03.556238       1 trace.go:236] Trace[779672623]: "Update" accept:application/json, */*,audit-id:f8de1766-1831-49ce-b601-b05c405cf9a4,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:10:02.819) (total time: 737ms):
Trace[779672623]: ["GuaranteedUpdate etcd3" audit-id:f8de1766-1831-49ce-b601-b05c405cf9a4,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 736ms (12:10:02.819)
Trace[779672623]:  ---"Txn call completed" 736ms (12:10:03.556)]
Trace[779672623]: [737.127372ms] [737.127372ms] END
I0809 12:10:56.654995       1 trace.go:236] Trace[1176258256]: "Update" accept:application/json, */*,audit-id:c20875cd-9b41-49e6-b821-f3d0630a6884,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:10:55.944) (total time: 710ms):
Trace[1176258256]: ["GuaranteedUpdate etcd3" audit-id:c20875cd-9b41-49e6-b821-f3d0630a6884,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 710ms (12:10:55.944)
Trace[1176258256]:  ---"Txn call completed" 709ms (12:10:56.654)]
Trace[1176258256]: [710.313415ms] [710.313415ms] END
I0809 12:11:21.466142       1 trace.go:236] Trace[1675832690]: "Update" accept:application/json, */*,audit-id:e15b2650-d3c5-41f3-814e-c52fe1dcee27,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Aug-2024 12:11:20.786) (total time: 679ms):
Trace[1675832690]: ["GuaranteedUpdate etcd3" audit-id:e15b2650-d3c5-41f3-814e-c52fe1dcee27,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 679ms (12:11:20.787)
Trace[1675832690]:  ---"Txn call completed" 678ms (12:11:21.465)]
Trace[1675832690]: [679.224813ms] [679.224813ms] END


==> kube-controller-manager [38dd7bac24a3] <==
I0809 11:20:14.681194       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="8.294489ms"
I0809 11:20:14.681416       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="43.202¬µs"
I0809 11:20:29.620925       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="38.202¬µs"
I0809 11:25:22.883397       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="19.257219ms"
I0809 11:25:22.883507       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="73.704¬µs"
I0809 11:25:41.044073       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="18.952404ms"
I0809 11:25:41.044156       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="46.702¬µs"
I0809 11:25:53.608521       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="35.902¬µs"
I0809 11:30:53.507647       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="65.389201ms"
I0809 11:30:53.507737       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="49.303¬µs"
I0809 11:31:11.610547       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="21.783148ms"
I0809 11:31:11.610627       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="37.402¬µs"
I0809 11:31:22.599955       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="40.103¬µs"
I0809 11:36:11.808214       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="22.366535ms"
I0809 11:36:11.808281       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="28.502¬µs"
I0809 11:36:29.961518       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="18.607326ms"
I0809 11:36:29.961578       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="32.402¬µs"
I0809 11:36:42.576179       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="42.002¬µs"
I0809 11:41:34.485641       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="28.191116ms"
I0809 11:41:34.485753       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="38.304¬µs"
I0809 11:41:53.681048       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="14.321793ms"
I0809 11:41:53.681295       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="39.5¬µs"
I0809 11:42:04.872746       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="36.602¬µs"
I0809 11:47:08.027109       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="26.259741ms"
I0809 11:47:08.027168       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="26.201¬µs"
I0809 11:47:25.189485       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="16.110545ms"
I0809 11:47:25.189595       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="39.202¬µs"
I0809 11:47:38.884931       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="43.902¬µs"
I0809 11:49:19.791716       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="42.902¬µs"
I0809 11:49:19.791716       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/postgres-deployment-66b6655f68" duration="51.503¬µs"
I0809 11:49:19.791811       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="74.105¬µs"
I0809 11:52:28.474845       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="18.849466ms"
I0809 11:52:28.475093       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="183.711¬µs"
I0809 11:52:48.046834       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="386.880114ms"
I0809 11:52:48.047008       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="34.402¬µs"
I0809 11:53:02.926702       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="43.103¬µs"
I0809 11:58:02.231011       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="37.834141ms"
I0809 11:58:02.231150       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="70.803¬µs"
I0809 11:58:25.457477       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="11.308709ms"
I0809 11:58:25.457620       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="38.902¬µs"
I0809 11:58:37.829748       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="42.402¬µs"
I0809 12:03:28.078115       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="20.696587ms"
I0809 12:03:28.078443       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="44.192¬µs"
I0809 12:03:48.264515       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="13.268616ms"
I0809 12:03:48.265439       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="27.901¬µs"
I0809 12:03:59.102977       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="34.501¬µs"
I0809 12:04:02.672403       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-5d46bd97cd" duration="132.17986ms"
I0809 12:04:02.715689       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-5d46bd97cd" duration="43.231345ms"
I0809 12:04:02.715750       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-5d46bd97cd" duration="31.766¬µs"
I0809 12:04:02.757012       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-5d46bd97cd" duration="36.277¬µs"
I0809 12:04:04.460921       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-5d46bd97cd" duration="9.734456ms"
I0809 12:04:04.460991       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-5d46bd97cd" duration="33.901¬µs"
I0809 12:04:04.518179       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="47.760965ms"
I0809 12:04:04.551494       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="33.278934ms"
I0809 12:04:04.551695       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="40.601¬µs"
I0809 12:04:04.562218       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="85.902¬µs"
I0809 12:04:04.785632       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="35.901¬µs"
I0809 12:04:05.489496       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="29.6¬µs"
I0809 12:04:05.507820       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="32.301¬µs"
I0809 12:04:05.523580       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/pg-admin-deployment-cbf865f66" duration="196.303¬µs"


==> kube-proxy [8e2e5b782910] <==
I0808 15:24:01.001397       1 server_linux.go:69] "Using iptables proxy"
I0808 15:24:01.096891       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
I0808 15:24:01.154528       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0808 15:24:01.154624       1 server_linux.go:165] "Using iptables Proxier"
I0808 15:24:01.159810       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0808 15:24:01.159896       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0808 15:24:01.159949       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0808 15:24:01.163097       1 server.go:872] "Version info" version="v1.30.0"
I0808 15:24:01.163144       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0808 15:24:01.170846       1 config.go:192] "Starting service config controller"
I0808 15:24:01.171004       1 shared_informer.go:313] Waiting for caches to sync for service config
I0808 15:24:01.173367       1 config.go:101] "Starting endpoint slice config controller"
I0808 15:24:01.173393       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0808 15:24:01.174178       1 config.go:319] "Starting node config controller"
I0808 15:24:01.174216       1 shared_informer.go:313] Waiting for caches to sync for node config
I0808 15:24:01.271784       1 shared_informer.go:320] Caches are synced for service config
I0808 15:24:01.274164       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0808 15:24:01.274278       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [a5fca69f982d] <==
E0808 15:23:19.602817       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:19.609550       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:19.609815       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:19.637120       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:19.637168       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:19.911078       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:19.911119       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:19.929912       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:19.942362       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:19.977665       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:19.977963       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:20.076555       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:20.076657       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:20.077192       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:20.077597       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:20.155620       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:20.155666       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:20.177074       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:20.177150       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:20.192773       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:20.192842       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:20.202820       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:20.202909       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:20.274187       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0808 15:23:20.274239       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0808 15:23:24.127608       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0808 15:23:24.143556       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0808 15:23:24.139023       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0808 15:23:24.139073       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0808 15:23:24.139112       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0808 15:23:24.139142       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0808 15:23:24.139168       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0808 15:23:24.139193       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0808 15:23:24.139274       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0808 15:23:24.139311       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0808 15:23:24.139344       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0808 15:23:24.139397       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0808 15:23:24.139442       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0808 15:23:24.139476       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0808 15:23:24.139511       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0808 15:23:24.146228       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0808 15:23:24.148351       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0808 15:23:24.152060       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0808 15:23:24.152826       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0808 15:23:24.154249       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0808 15:23:24.154635       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0808 15:23:24.154987       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0808 15:23:24.155174       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0808 15:23:24.155430       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0808 15:23:24.155665       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0808 15:23:24.155735       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0808 15:23:24.156130       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0808 15:23:24.156201       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0808 15:23:24.159653       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0808 15:23:24.161057       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0808 15:23:27.521005       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0808 15:23:27.521054       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0808 15:23:27.596878       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0808 15:23:27.598754       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0808 15:23:35.965577       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Aug 09 11:58:25 minikube kubelet[2293]: E0809 11:58:25.426533    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 11:58:37 minikube kubelet[2293]: I0809 11:58:37.813063    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 11:58:37 minikube kubelet[2293]: E0809 11:58:37.813382    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 11:58:50 minikube kubelet[2293]: I0809 11:58:50.813532    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 11:58:50 minikube kubelet[2293]: E0809 11:58:50.813800    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 11:59:05 minikube kubelet[2293]: I0809 11:59:05.811098    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 11:59:05 minikube kubelet[2293]: E0809 11:59:05.811412    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 11:59:19 minikube kubelet[2293]: I0809 11:59:19.811492    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 11:59:19 minikube kubelet[2293]: E0809 11:59:19.811805    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 11:59:32 minikube kubelet[2293]: I0809 11:59:32.810975    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 11:59:32 minikube kubelet[2293]: E0809 11:59:32.811237    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 11:59:44 minikube kubelet[2293]: I0809 11:59:44.810804    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 11:59:44 minikube kubelet[2293]: E0809 11:59:44.811127    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 11:59:56 minikube kubelet[2293]: I0809 11:59:56.809766    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 11:59:56 minikube kubelet[2293]: E0809 11:59:56.810041    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:00:11 minikube kubelet[2293]: I0809 12:00:11.808797    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:00:11 minikube kubelet[2293]: E0809 12:00:11.809098    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:00:26 minikube kubelet[2293]: I0809 12:00:26.809937    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:00:26 minikube kubelet[2293]: E0809 12:00:26.810322    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:00:40 minikube kubelet[2293]: I0809 12:00:40.807576    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:00:40 minikube kubelet[2293]: E0809 12:00:40.807841    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:00:55 minikube kubelet[2293]: I0809 12:00:55.796640    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:00:55 minikube kubelet[2293]: E0809 12:00:55.797244    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:01:07 minikube kubelet[2293]: I0809 12:01:07.796662    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:01:07 minikube kubelet[2293]: E0809 12:01:07.797089    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:01:19 minikube kubelet[2293]: I0809 12:01:19.797438    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:01:19 minikube kubelet[2293]: E0809 12:01:19.797778    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:01:32 minikube kubelet[2293]: I0809 12:01:32.794441    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:01:32 minikube kubelet[2293]: E0809 12:01:32.794749    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:01:46 minikube kubelet[2293]: I0809 12:01:46.793858    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:01:46 minikube kubelet[2293]: E0809 12:01:46.794140    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:01:58 minikube kubelet[2293]: I0809 12:01:58.793605    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:01:58 minikube kubelet[2293]: E0809 12:01:58.793863    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:02:12 minikube kubelet[2293]: I0809 12:02:12.793975    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:02:12 minikube kubelet[2293]: E0809 12:02:12.794253    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:02:26 minikube kubelet[2293]: I0809 12:02:26.793068    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:02:26 minikube kubelet[2293]: E0809 12:02:26.793360    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:02:41 minikube kubelet[2293]: I0809 12:02:41.792663    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:02:41 minikube kubelet[2293]: E0809 12:02:41.792966    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:02:52 minikube kubelet[2293]: I0809 12:02:52.793539    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:02:52 minikube kubelet[2293]: E0809 12:02:52.794078    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:03:04 minikube kubelet[2293]: I0809 12:03:04.082400    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:03:04 minikube kubelet[2293]: E0809 12:03:04.082789    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:03:16 minikube kubelet[2293]: I0809 12:03:16.082523    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:03:16 minikube kubelet[2293]: E0809 12:03:16.082849    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:03:27 minikube kubelet[2293]: I0809 12:03:27.153332    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:03:48 minikube kubelet[2293]: I0809 12:03:48.234024    2293 scope.go:117] "RemoveContainer" containerID="2eb3b50f284028b3803ec420a80a132477ba464e33947eebce2510d1cd7c5c16"
Aug 09 12:03:48 minikube kubelet[2293]: I0809 12:03:48.234410    2293 scope.go:117] "RemoveContainer" containerID="d07abcfa184a91c717243fff70be46627cb3eb701d46a6b96587a0a88eb74692"
Aug 09 12:03:48 minikube kubelet[2293]: E0809 12:03:48.234643    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:03:59 minikube kubelet[2293]: I0809 12:03:59.080684    2293 scope.go:117] "RemoveContainer" containerID="d07abcfa184a91c717243fff70be46627cb3eb701d46a6b96587a0a88eb74692"
Aug 09 12:03:59 minikube kubelet[2293]: E0809 12:03:59.081561    2293 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"pg-admin\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=pg-admin pod=pg-admin-deployment-cbf865f66-g9cpr_default(c899c993-06e5-4562-aa82-775d4438575e)\"" pod="default/pg-admin-deployment-cbf865f66-g9cpr" podUID="c899c993-06e5-4562-aa82-775d4438575e"
Aug 09 12:04:02 minikube kubelet[2293]: I0809 12:04:02.667907    2293 topology_manager.go:215] "Topology Admit Handler" podUID="8bf66f0e-e812-4e4d-a71c-f29a212d8277" podNamespace="default" podName="pg-admin-deployment-5d46bd97cd-sn22v"
Aug 09 12:04:02 minikube kubelet[2293]: I0809 12:04:02.801058    2293 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mzjxk\" (UniqueName: \"kubernetes.io/projected/8bf66f0e-e812-4e4d-a71c-f29a212d8277-kube-api-access-mzjxk\") pod \"pg-admin-deployment-5d46bd97cd-sn22v\" (UID: \"8bf66f0e-e812-4e4d-a71c-f29a212d8277\") " pod="default/pg-admin-deployment-5d46bd97cd-sn22v"
Aug 09 12:04:03 minikube kubelet[2293]: I0809 12:04:03.401334    2293 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ff01392e996743dda5b3c8ced9011272d0f2d89ec22b82692bc27518c0d35633"
Aug 09 12:04:04 minikube kubelet[2293]: I0809 12:04:04.494838    2293 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/pg-admin-deployment-5d46bd97cd-sn22v" podStartSLOduration=2.49277979 podStartE2EDuration="2.49277979s" podCreationTimestamp="2024-08-09 12:04:02 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-08-09 12:04:04.454978784 +0000 UTC m=+74425.885711938" watchObservedRunningTime="2024-08-09 12:04:04.49277979 +0000 UTC m=+74425.923512844"
Aug 09 12:04:04 minikube kubelet[2293]: I0809 12:04:04.916796    2293 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sgrzq\" (UniqueName: \"kubernetes.io/projected/c899c993-06e5-4562-aa82-775d4438575e-kube-api-access-sgrzq\") pod \"c899c993-06e5-4562-aa82-775d4438575e\" (UID: \"c899c993-06e5-4562-aa82-775d4438575e\") "
Aug 09 12:04:04 minikube kubelet[2293]: I0809 12:04:04.931870    2293 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c899c993-06e5-4562-aa82-775d4438575e-kube-api-access-sgrzq" (OuterVolumeSpecName: "kube-api-access-sgrzq") pod "c899c993-06e5-4562-aa82-775d4438575e" (UID: "c899c993-06e5-4562-aa82-775d4438575e"). InnerVolumeSpecName "kube-api-access-sgrzq". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 09 12:04:05 minikube kubelet[2293]: I0809 12:04:05.017325    2293 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-sgrzq\" (UniqueName: \"kubernetes.io/projected/c899c993-06e5-4562-aa82-775d4438575e-kube-api-access-sgrzq\") on node \"minikube\" DevicePath \"\""
Aug 09 12:04:05 minikube kubelet[2293]: I0809 12:04:05.445597    2293 scope.go:117] "RemoveContainer" containerID="d07abcfa184a91c717243fff70be46627cb3eb701d46a6b96587a0a88eb74692"
Aug 09 12:04:07 minikube kubelet[2293]: I0809 12:04:07.087423    2293 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="c899c993-06e5-4562-aa82-775d4438575e" path="/var/lib/kubelet/pods/c899c993-06e5-4562-aa82-775d4438575e/volumes"


==> storage-provisioner [4779a4ba40e2] <==
I0808 15:24:00.308851       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0808 15:24:21.359659       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [889a73a87362] <==
I0808 15:24:23.486101       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0808 15:24:23.495872       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0808 15:24:23.495957       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0808 15:24:23.569993       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0808 15:24:23.570199       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a751691e-90ed-40af-b201-3f007cf89fb8", APIVersion:"v1", ResourceVersion:"433", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_3605f481-98f8-4b31-944a-e171c39e41a8 became leader
I0808 15:24:23.570199       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_3605f481-98f8-4b31-944a-e171c39e41a8!
I0808 15:24:23.670694       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_3605f481-98f8-4b31-944a-e171c39e41a8!

